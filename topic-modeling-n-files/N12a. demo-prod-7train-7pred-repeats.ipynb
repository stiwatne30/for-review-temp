{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE - Run data/download_datasets.ipynb if not already done, to download all data files required to run this file\n",
    "\n",
    "## Description:\n",
    "\n",
    "This file contains:\n",
    "* **Opt-1: train on past 7 days eligible articles - same model - predict upto 7 days including all articles repeating from past days** \n",
    "    * with HP-tuned and noise-reduction via cosine and topic_size=1 articles assigned to noise\n",
    "\n",
    "This exp is being done for the purposes of:\n",
    "* .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted - show topic, distinct tier describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Helper imports\"\"\"\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from time import time\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\"\"\"Modeling imports\"\"\"\n",
    "import umap\n",
    "import hdbscan\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from hdbscan import validity_index\n",
    "\n",
    "\"\"\" Plotting imports\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "# remove this later\n",
    "pd.set_option('max_colwidth', 110)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# add the parent directory path\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from src.util import (c_tf_idf, \n",
    "                      extract_top_n_words_per_topic,\n",
    "                      extract_topic_sizes,\n",
    "                      topic_cos_sim_metrics,\n",
    "                      generate_all_cossim_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = '/home/jupyter/bi-topic-modeling/data/' \n",
    "results_path = '/home/jupyter/bi-topic-modeling/results/' \n",
    "\n",
    "SEED = 42\n",
    "\n",
    "STOP_WORDS = pd.read_pickle(data_path + 'forbes_stop_words.pkl')\n",
    "\"q1\" in STOP_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --- Data ---\n",
    "\n",
    "* Train date range Apr 26 (monday) till May 02 (sunday)\n",
    "* Eligible pool = articles with pvs > 100 on a given day. In prod this will be replaced by pvs>350 summed over 7 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(file_name):\n",
    "    \n",
    "    df = pd.read_csv(data_path + file_name)\n",
    "    \n",
    "    # Per verdict from EDA, exclude articles less than 100 words\n",
    "    df[\"article_length\"] = df.clean_body.str.split().apply(len)\n",
    "    df = df[df[\"article_length\"]>=100]\n",
    "    print(\"Shape:\", df.shape)\n",
    "    \n",
    "    df.tier1 = df.tier1.fillna('Other')\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- apr 25 ---\n",
      "Shape: (3327, 11)\n",
      "Embeds len: 3327\n",
      "\n",
      "--- apr 26 ---\n",
      "Shape: (4128, 11)\n",
      "Embeds len: 4128\n",
      "\n",
      "--- apr 27 ---\n",
      "Shape: (4167, 11)\n",
      "Embeds len: 4167\n",
      "\n",
      "--- apr 28 ---\n",
      "Shape: (4186, 9)\n",
      "Embeds len: 4186\n",
      "\n",
      "--- apr 29 ---\n",
      "Shape: (4188, 9)\n",
      "Embeds len: 4188\n",
      "\n",
      "--- apr 30 ---\n",
      "Shape: (3771, 9)\n",
      "Embeds len: 3771\n",
      "--- may 01 ---\n",
      "Shape: (3044, 9)\n",
      "Embeds len: 3044\n",
      "\n",
      "--- may 02 ---\n",
      "Shape: (3259, 9)\n",
      "Embeds len: 3259\n"
     ]
    }
   ],
   "source": [
    "# training data set\n",
    "\n",
    "print(\"--- apr 25 ---\")\n",
    "apr25 = get_data('processed/train_apr25_over100pvs_processed.csv')\n",
    "apr25_art_embed = joblib.load(data_path + \"/processed/apr25_article_embed_axis0.joblib\")\n",
    "print(\"Embeds len:\", len(apr25_art_embed))\n",
    "\n",
    "print()\n",
    "print(\"--- apr 26 ---\")\n",
    "apr26 = get_data('processed/pred_apr26_over100pvs_processed.csv')\n",
    "apr26_art_embed = joblib.load(data_path + \"processed/apr26_article_embed_axis0.joblib\")\n",
    "print(\"Embeds len:\", len(apr26_art_embed))\n",
    "\n",
    "print()\n",
    "print(\"--- apr 27 ---\")\n",
    "apr27 = get_data('processed/pred_apr27_over100pvs_processed.csv')\n",
    "apr27_art_embed = joblib.load(data_path + \"processed/apr27_article_embed_axis0.joblib\")\n",
    "print(\"Embeds len:\", len(apr27_art_embed))\n",
    "\n",
    "print()\n",
    "print(\"--- apr 28 ---\")\n",
    "apr28 = get_data('processed/apr28_over100pvs_processed.csv')\n",
    "apr28_art_embed = joblib.load(data_path + \"processed/apr28_article_embed_axis0.joblib\")\n",
    "print(\"Embeds len:\", len(apr28_art_embed))\n",
    "\n",
    "print()\n",
    "print(\"--- apr 29 ---\")\n",
    "apr29 = get_data('processed/apr29_over100pvs_processed.csv')\n",
    "apr29_art_embed = joblib.load(data_path + \"processed/apr29_article_embed_axis0.joblib\")\n",
    "print(\"Embeds len:\", len(apr29_art_embed))\n",
    "\n",
    "print()\n",
    "print(\"--- apr 30 ---\")\n",
    "apr30 = get_data('processed/apr30_over100pvs_processed.csv')\n",
    "apr30_art_embed = joblib.load(data_path + \"processed/apr30_article_embed_axis0.joblib\")\n",
    "print(\"Embeds len:\", len(apr30_art_embed))\n",
    "\n",
    "print(\"--- may 01 ---\")\n",
    "may01 = get_data('processed/may01_over100pvs_processed.csv')\n",
    "may01_art_embed = joblib.load(data_path + \"processed/may01_article_embed_axis0.joblib\")\n",
    "print(\"Embeds len:\", len(may01_art_embed))\n",
    "\n",
    "print()\n",
    "print(\"--- may 02 ---\")\n",
    "may02 = get_data('processed/may02_over100pvs_processed.csv')\n",
    "may02_art_embed = joblib.load(data_path + \"processed/may02_article_embed_axis0.joblib\")\n",
    "print(\"Embeds len:\", len(may02_art_embed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- may 03 ---\n",
      "Shape: (4199, 9)\n",
      "Embeds len: 4199\n",
      "\n",
      "--- may 04 ---\n",
      "Shape: (4223, 9)\n",
      "Embeds len: 4223\n",
      "\n",
      "--- may 05 ---\n",
      "Shape: (4160, 9)\n",
      "Embeds len: 4160\n",
      "\n",
      "--- may 06 ---\n",
      "Shape: (4063, 9)\n",
      "Embeds len: 4063\n",
      "\n",
      "--- may 07 ---\n",
      "Shape: (3630, 9)\n",
      "Embeds len: 3630\n",
      "\n",
      "--- may 08 ---\n",
      "Shape: (3025, 9)\n",
      "Embeds len: 3025\n",
      "\n",
      "--- may 09 ---\n",
      "Shape: (3231, 9)\n",
      "Embeds len: 3231\n"
     ]
    }
   ],
   "source": [
    "# testing data set\n",
    "\n",
    "print()\n",
    "print(\"--- may 03 ---\")\n",
    "may03 = get_data('processed/may03_over100pvs_processed.csv')\n",
    "may03_art_embed = joblib.load(data_path + \"processed/may03_article_embed_axis0.joblib\")\n",
    "print(\"Embeds len:\", len(may03_art_embed))\n",
    "\n",
    "print()\n",
    "print(\"--- may 04 ---\")\n",
    "may04 = get_data('processed/may04_over100pvs_processed.csv')\n",
    "may04_art_embed = joblib.load(data_path + \"processed/may04_article_embed_axis0.joblib\")\n",
    "print(\"Embeds len:\", len(may04_art_embed))\n",
    "\n",
    "print()\n",
    "print(\"--- may 05 ---\")\n",
    "may05 = get_data('processed/may05_over100pvs_processed.csv')\n",
    "may05_art_embed = joblib.load(data_path + \"processed/may05_article_embed_axis0.joblib\")\n",
    "print(\"Embeds len:\", len(may05_art_embed))\n",
    "\n",
    "print()\n",
    "print(\"--- may 06 ---\")\n",
    "may06 = get_data('processed/may06_over100pvs_processed.csv')\n",
    "may06_art_embed = joblib.load(data_path + \"processed/may06_article_embed_axis0.joblib\")\n",
    "print(\"Embeds len:\", len(may06_art_embed))\n",
    "\n",
    "print()\n",
    "print(\"--- may 07 ---\")\n",
    "may07 = get_data('processed/may07_over100pvs_processed.csv')\n",
    "may07_art_embed = joblib.load(data_path + \"processed/may07_article_embed_axis0.joblib\")\n",
    "print(\"Embeds len:\", len(may07_art_embed))\n",
    "\n",
    "print()\n",
    "print(\"--- may 08 ---\")\n",
    "may08 = get_data('processed/may08_over100pvs_processed.csv')\n",
    "may08_art_embed = joblib.load(data_path + \"processed/may08_article_embed_axis0.joblib\")\n",
    "print(\"Embeds len:\", len(may08_art_embed))\n",
    "\n",
    "print()\n",
    "print(\"--- may 09 ---\")\n",
    "may09 = get_data('processed/may09_over100pvs_processed.csv')\n",
    "may09_art_embed = joblib.load(data_path + \"processed/may09_article_embed_axis0.joblib\")\n",
    "print(\"Embeds len:\", len(may09_art_embed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opt-1\n",
    "\n",
    "* train on past 7 days eligible articles - same model - predict upto 7 days including all articles repeating from past days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --- Combine train data - 7 days --- \n",
    "* apr 26 (monday) till may 2 (sunday)\n",
    "* Observations on training clustering performance - same as the one in file shared for review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26743, 9)\n",
      "26743\n"
     ]
    }
   ],
   "source": [
    "# concatenate dfs\n",
    "\n",
    "combined_df = pd.concat([apr26, apr27, apr28, apr29, apr30, may01, may02])\n",
    "# reset index\n",
    "combined_df = combined_df.reset_index(drop=True)\n",
    "# drop unnecessary cols\n",
    "combined_df.drop([\"processed\", \"processed_noun\", \"processed_noun_2\"], axis = 1, inplace=True)\n",
    "# fill na\n",
    "combined_df.tier2 = combined_df.tier2.fillna('Other')\n",
    "combined_df.total_timeonpage = combined_df.total_timeonpage.fillna(0)\n",
    "print(combined_df.shape)\n",
    "\n",
    "# concatenate embeds\n",
    "combined_embeds = apr26_art_embed + apr27_art_embed + apr28_art_embed + apr29_art_embed + apr30_art_embed + may01_art_embed + may02_art_embed\n",
    "print(len(combined_embeds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7238, 9)\n",
      "False\n",
      "7238\n"
     ]
    }
   ],
   "source": [
    "# drop duplicates from df\n",
    "\n",
    "train_df = combined_df.drop_duplicates(subset=\"content_natid\", keep='first')\n",
    "\n",
    "print(train_df.shape)\n",
    "print(train_df.content_natid.duplicated(keep='first').any())\n",
    "\n",
    "train_embeds =  [combined_embeds[i] for i in list(train_df.index)]\n",
    "print(len(train_embeds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7238, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "train_df = train_df[['content_natid', 'clean_body', 'tier1']]\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --- HP-tuned Modeling ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7238\n"
     ]
    }
   ],
   "source": [
    "data = train_df.clean_body.tolist()\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_dbcv_score(embeds, pred_labels):\n",
    "    return validity_index(embeds, pred_labels)\n",
    "\n",
    "def evaluate_params(train_embeds, min_dist_params, n_neighbors_params, n_components_params, \n",
    "                    min_cluster_size_params, min_samples_params):\n",
    "    \n",
    "    # initialize\n",
    "    best_score, best_cfg = 0, None\n",
    "    \n",
    "    # manually loop over parameter lists\n",
    "    for mdis in min_dist_params:\n",
    "        for neigh in n_neighbors_params:\n",
    "            for comp in n_components_params:\n",
    "                for clust in min_cluster_size_params:\n",
    "                    for samp in min_samples_params:\n",
    "\n",
    "                        # fit algo with this config\n",
    "                        order = (mdis, neigh, comp, clust, samp)\n",
    "\n",
    "                        try:\n",
    "                            # reduce dimensions\n",
    "                            umap_embeddings_hp = umap.UMAP(n_neighbors= neigh,\n",
    "                                 min_dist = mdis,\n",
    "                                 n_components = comp,\n",
    "                                 random_state = SEED,\n",
    "                                ).fit(train_embeds)\n",
    "\n",
    "                            # cluster\n",
    "                            cluster_hp = hdbscan.HDBSCAN(min_cluster_size = clust,\n",
    "                                                  min_samples = samp,\n",
    "                                                  metric= 'euclidean',                      \n",
    "                                                  cluster_selection_method='eom',\n",
    "                                                  prediction_data=True).fit(umap_embeddings_hp.embedding_)\n",
    "\n",
    "                            # calculate dbcv\n",
    "                            labels = cluster_hp.labels_\n",
    "                            score = evaluate_dbcv_score(umap_embeddings_hp.embedding_.astype('double'), labels)\n",
    "\n",
    "                            print('params%s score=%.3f' % (order, score))\n",
    "\n",
    "                            # optimize dbcv - the bigger the better\n",
    "                            if score > best_score:\n",
    "                                best_score, best_cfg = score, order\n",
    "                        except:\n",
    "                            continue\n",
    "    print('Best params=%s Best Score=%.3f' % ( best_cfg, best_score ))\n",
    "    \n",
    "    # return best configuration\n",
    "    return best_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_dist_params = [0.0, 0.1] \n",
    "n_neighbors_params = [12, 15, 20] \n",
    "n_components_params = [2, 5] \n",
    "min_cluster_size_params = [5, 10] \n",
    "min_samples_params = [4, 5] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params(0.0, 12, 2, 5, 4) score=0.328\n",
      "params(0.0, 12, 2, 5, 5) score=0.341\n",
      "params(0.0, 12, 2, 10, 4) score=0.241\n",
      "params(0.0, 12, 2, 10, 5) score=0.250\n",
      "params(0.0, 12, 5, 5, 4) score=0.318\n",
      "params(0.0, 12, 5, 5, 5) score=0.318\n",
      "params(0.0, 12, 5, 10, 4) score=0.253\n",
      "params(0.0, 12, 5, 10, 5) score=0.272\n",
      "params(0.0, 15, 2, 5, 4) score=0.300\n",
      "params(0.0, 15, 2, 5, 5) score=0.328\n",
      "params(0.0, 15, 2, 10, 4) score=0.224\n",
      "params(0.0, 15, 2, 10, 5) score=0.282\n",
      "params(0.0, 15, 5, 5, 4) score=0.304\n",
      "params(0.0, 15, 5, 5, 5) score=0.305\n",
      "params(0.0, 15, 5, 10, 4) score=0.224\n",
      "params(0.0, 15, 5, 10, 5) score=0.235\n",
      "params(0.0, 20, 2, 5, 4) score=0.261\n",
      "params(0.0, 20, 2, 5, 5) score=0.295\n",
      "params(0.0, 20, 2, 10, 4) score=0.239\n",
      "params(0.0, 20, 2, 10, 5) score=0.244\n",
      "params(0.0, 20, 5, 5, 4) score=0.268\n",
      "params(0.0, 20, 5, 5, 5) score=0.283\n",
      "params(0.0, 20, 5, 10, 4) score=0.230\n",
      "params(0.0, 20, 5, 10, 5) score=0.255\n",
      "params(0.1, 12, 2, 5, 4) score=0.277\n",
      "params(0.1, 12, 2, 5, 5) score=0.289\n",
      "params(0.1, 12, 2, 10, 4) score=0.213\n",
      "params(0.1, 12, 2, 10, 5) score=0.221\n",
      "params(0.1, 12, 5, 5, 4) score=0.251\n",
      "params(0.1, 12, 5, 5, 5) score=0.245\n",
      "params(0.1, 12, 5, 10, 4) score=0.172\n",
      "params(0.1, 12, 5, 10, 5) score=0.187\n",
      "params(0.1, 15, 2, 5, 4) score=0.251\n",
      "params(0.1, 15, 2, 5, 5) score=0.245\n",
      "params(0.1, 15, 2, 10, 4) score=0.163\n",
      "params(0.1, 15, 2, 10, 5) score=0.210\n",
      "params(0.1, 15, 5, 5, 4) score=0.233\n",
      "params(0.1, 15, 5, 5, 5) score=0.222\n",
      "params(0.1, 15, 5, 10, 4) score=0.172\n",
      "params(0.1, 15, 5, 10, 5) score=0.196\n",
      "params(0.1, 20, 2, 5, 4) score=0.261\n",
      "params(0.1, 20, 2, 5, 5) score=0.254\n",
      "params(0.1, 20, 2, 10, 4) score=0.197\n",
      "params(0.1, 20, 2, 10, 5) score=0.162\n",
      "params(0.1, 20, 5, 5, 4) score=0.218\n",
      "params(0.1, 20, 5, 5, 5) score=0.241\n",
      "params(0.1, 20, 5, 10, 4) score=0.191\n",
      "params(0.1, 20, 5, 10, 5) score=0.198\n",
      "Best params=(0.0, 12, 2, 5, 5) Best Score=0.341\n",
      "\n",
      "time (mins) 15.6\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "best_cfg = evaluate_params(train_embeds, \n",
    "                           min_dist_params,\n",
    "                           n_neighbors_params, \n",
    "                           n_components_params, \n",
    "                           min_cluster_size_params, \n",
    "                           min_samples_params)\n",
    "\n",
    "print()\n",
    "end = time.time()\n",
    "print(\"time (mins)\", round((end - start)/60, 2)) # 15mins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Fit model with tuned parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_cfg = (0.0, 12, 2, 5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.0, 12, 2, 5, 5)\n"
     ]
    }
   ],
   "source": [
    "print(best_cfg)\n",
    "\n",
    "# dimensionality reduction\n",
    "umap_embeddings =  umap.UMAP(min_dist = best_cfg[0],\n",
    "                             n_neighbors = best_cfg[1],\n",
    "                             n_components= best_cfg[2],\n",
    "                             random_state=SEED,\n",
    "                            ).fit(train_embeds)\n",
    "\n",
    "# cluster\n",
    "cluster = hdbscan.HDBSCAN(min_cluster_size= best_cfg[3],\n",
    "                      min_samples = best_cfg[4],\n",
    "                      metric= 'euclidean',                      \n",
    "                      cluster_selection_method='eom',\n",
    "                      prediction_data=True).fit(umap_embeddings.embedding_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training embeddings: (7238, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of training embeddings:\", umap_embeddings.embedding_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings - mean axis=0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.34083637100782105"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = cluster.labels_\n",
    "print(\"Settings - mean axis=0\")\n",
    "validity_index(umap_embeddings.embedding_.astype('double'), labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct topics including -1 for noise 280\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Doc_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10 robot vacuums that clean so you dont have to. if youre like most people, vacuuming is low on your list ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>these 11 mattress toppers give you the bed of your dreams. while new pillows or a new set of sheets can do...</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wayfairs biggest sale of the year is officially here. thanks to the pandemic, most of us have spent the pa...</td>\n",
       "      <td>70</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>apple ios 14.5 released: massive iphone update with cool features &amp; important fixes. april 28 update below...</td>\n",
       "      <td>60</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>apple airtag: all you need to know about the game-changing new gadget. april 28 update below. this post wa...</td>\n",
       "      <td>-1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7233</th>\n",
       "      <td>the oilman, the playmate, and the tangled affairs of the billionaire marshall family. j. howard marshall i...</td>\n",
       "      <td>140</td>\n",
       "      <td>7233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7234</th>\n",
       "      <td>5 surprising foods that help you sleep. cherries: the new melatonin supplement (image via wikipedia) june ...</td>\n",
       "      <td>35</td>\n",
       "      <td>7234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7235</th>\n",
       "      <td>hacker's demo shows how easily credit cards can be read through clothes and wallets. some blank credit car...</td>\n",
       "      <td>31</td>\n",
       "      <td>7235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7236</th>\n",
       "      <td>the five richest pastors in nigeria. london-based nigerian pastor matthew ashimolowo god is good, especial...</td>\n",
       "      <td>172</td>\n",
       "      <td>7236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7237</th>\n",
       "      <td>man inadvertently live tweets osama bin laden raid. an it consultant who tried to take a break from the ra...</td>\n",
       "      <td>-1</td>\n",
       "      <td>7237</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7238 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                Doc  \\\n",
       "0     10 robot vacuums that clean so you dont have to. if youre like most people, vacuuming is low on your list ...   \n",
       "1     these 11 mattress toppers give you the bed of your dreams. while new pillows or a new set of sheets can do...   \n",
       "2     wayfairs biggest sale of the year is officially here. thanks to the pandemic, most of us have spent the pa...   \n",
       "3     apple ios 14.5 released: massive iphone update with cool features & important fixes. april 28 update below...   \n",
       "4     apple airtag: all you need to know about the game-changing new gadget. april 28 update below. this post wa...   \n",
       "...                                                                                                             ...   \n",
       "7233  the oilman, the playmate, and the tangled affairs of the billionaire marshall family. j. howard marshall i...   \n",
       "7234  5 surprising foods that help you sleep. cherries: the new melatonin supplement (image via wikipedia) june ...   \n",
       "7235  hacker's demo shows how easily credit cards can be read through clothes and wallets. some blank credit car...   \n",
       "7236  the five richest pastors in nigeria. london-based nigerian pastor matthew ashimolowo god is good, especial...   \n",
       "7237  man inadvertently live tweets osama bin laden raid. an it consultant who tried to take a break from the ra...   \n",
       "\n",
       "      Topic  Doc_ID  \n",
       "0        -1       0  \n",
       "1        64       1  \n",
       "2        70       2  \n",
       "3        60       3  \n",
       "4        -1       4  \n",
       "...     ...     ...  \n",
       "7233    140    7233  \n",
       "7234     35    7234  \n",
       "7235     31    7235  \n",
       "7236    172    7236  \n",
       "7237     -1    7237  \n",
       "\n",
       "[7238 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# put topics in dataframe\n",
    "docs_df = pd.DataFrame(data, columns=[\"Doc\"])\n",
    "docs_df['Topic'] = cluster.labels_\n",
    "docs_df['Doc_ID'] = range(len(docs_df))\n",
    "\n",
    "print(\"Distinct topics including -1 for noise\", len(docs_df.Topic.unique()))\n",
    "print()\n",
    "docs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine together all articles belonging to same cluster  \n",
    "docs_per_topic = docs_df.groupby(['Topic'], as_index = False).agg({'Doc': ' '.join})\n",
    "\n",
    "# calculate countVector and tf-idf\n",
    "tf_idf, count = c_tf_idf(docs_per_topic.Doc.values, m=len(data))\n",
    "\n",
    "# top n words, topic size\n",
    "top_n_words = extract_top_n_words_per_topic(tf_idf, count, docs_per_topic, n=20)\n",
    "topic_sizes = extract_topic_sizes(docs_df); \n",
    "\n",
    "# modify top_n_words per topic dict into dataframe for presentation\n",
    "topic_words_dict = {}  \n",
    "\n",
    "for k, v in top_n_words.items():\n",
    "    topic_words_dict[k] = \", \".join([elem[0] for elem in v])\n",
    "\n",
    "pd.set_option(\"max_colwidth\", 250)\n",
    "top_n_words_df = pd.DataFrame.from_dict(topic_words_dict, \n",
    "                                        orient='index').reset_index().rename(columns={'index':'topic_num',\n",
    "                                                                                      \n",
    "                                                                                      0:'topic_words'}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keyword time 0.0004668235778808594\n"
     ]
    }
   ],
   "source": [
    "# clean keywords\n",
    "\n",
    "from collections import OrderedDict\n",
    "# from fuzzywuzzy import process, fuzz\n",
    "from itertools import chain\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "clean_keys = []\n",
    "for index, row in top_n_words_df.iterrows(): \n",
    "    \n",
    "    # remove STOP WORDS and remove whitespaces\n",
    "    inter_list = [x.strip() for x in row[\"topic_words\"].split(\",\") if x not in STOP_WORDS]\n",
    "    \n",
    "    # remove numbers but not alphanumerics e.g. remove 400, not ps4 \n",
    "    inter_list = [x for x in inter_list if not x.isnumeric()]\n",
    "    \n",
    "    # remove subset-based similar words - e.g. 'pcr tests' and 'tests' then 'tests' will be removed\n",
    "    inter_list = [i for i in inter_list if not any(i in x and i!=x for x in inter_list)]\n",
    "\n",
    "    # remove empty or 1-letter words\n",
    "    inter_list = [x for x in inter_list if len(x)>1]\n",
    "    \n",
    "    # de-duplicate\n",
    "    inter_list = list(OrderedDict.fromkeys(inter_list))\n",
    "    \n",
    "    # keep top 6 cleaned keywords\n",
    "    inter_list = inter_list[:6] \n",
    "    \n",
    "    # convert into string\n",
    "    inter_list = ', '.join(inter_list)\n",
    "    \n",
    "    clean_keys.append(inter_list)\n",
    "\n",
    "end = time.time()\n",
    "print(\"keyword time\", (end - start)/60) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_words_df[\"topic_words\"] = clean_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATE TOPIC METRICS\n",
    "\n",
    "# per topic - cos_min and cos_mean\n",
    "train_cossim_df = generate_all_cossim_metrics(docs_df, train_embeds)\n",
    "\n",
    "# per topic - tier counts\n",
    "tier_metric = docs_df.join(train_df[[\"tier1\"]])\n",
    "\n",
    "iab_metric = tier_metric.groupby(\"Topic\").agg({\"tier1\": \n",
    "                                               'nunique'}).round(2).reset_index().rename(columns = {'tier1': \n",
    "                                                                                                    'distinct_tier1s',\n",
    "                                                                                                    'Topic':\n",
    "                                                                                                    'topic_num'})\n",
    "\n",
    "# combine topic, top words, cos and tier metrics\n",
    "topic_info = pd.merge(pd.merge(pd.merge(top_n_words_df, \n",
    "                               topic_sizes, how=\"inner\", left_on=\"topic_num\", right_on = \"Topic\"), \n",
    "                               train_cossim_df, on = \"topic_num\", how=\"inner\"), \n",
    "                      iab_metric, on = \"topic_num\", how=\"inner\")\n",
    "\n",
    "topic_info.drop('Topic', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_num</th>\n",
       "      <th>topic_words</th>\n",
       "      <th>Size</th>\n",
       "      <th>cos_min</th>\n",
       "      <th>cos_mean</th>\n",
       "      <th>distinct_tier1s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>coffee, age, china, government, leaders, women</td>\n",
       "      <td>2357</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.72</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>10 states, average annual, annual wage, salaries, occupational, therapist salary</td>\n",
       "      <td>34</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.90</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>kentucky derby, essential quality, horses, churchill downs, race, triple crown</td>\n",
       "      <td>18</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.85</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>million viewers, 000 viewers, fox news, msnbc, cable news, prime time</td>\n",
       "      <td>7</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.97</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>packers, quarterback, round, rodgers, nfl draft, green bay</td>\n",
       "      <td>69</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.88</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   topic_num  \\\n",
       "0         -1   \n",
       "1          0   \n",
       "2          1   \n",
       "3          2   \n",
       "4          3   \n",
       "\n",
       "                                                                        topic_words  \\\n",
       "0                                    coffee, age, china, government, leaders, women   \n",
       "1  10 states, average annual, annual wage, salaries, occupational, therapist salary   \n",
       "2    kentucky derby, essential quality, horses, churchill downs, race, triple crown   \n",
       "3             million viewers, 000 viewers, fox news, msnbc, cable news, prime time   \n",
       "4                        packers, quarterback, round, rodgers, nfl draft, green bay   \n",
       "\n",
       "   Size  cos_min  cos_mean  distinct_tier1s  \n",
       "0  2357    -0.03      0.72               31  \n",
       "1    34     0.77      0.90                8  \n",
       "2    18     0.66      0.85                4  \n",
       "3     7     0.95      0.97                3  \n",
       "4    69     0.68      0.88                3  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cos_min</th>\n",
       "      <th>cos_mean</th>\n",
       "      <th>distinct_tier1s</th>\n",
       "      <th>Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>280.000</td>\n",
       "      <td>280.000</td>\n",
       "      <td>280.000</td>\n",
       "      <td>280.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.755</td>\n",
       "      <td>0.879</td>\n",
       "      <td>4.636</td>\n",
       "      <td>25.850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.091</td>\n",
       "      <td>0.034</td>\n",
       "      <td>3.003</td>\n",
       "      <td>140.567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.030</td>\n",
       "      <td>0.720</td>\n",
       "      <td>1.000</td>\n",
       "      <td>5.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.710</td>\n",
       "      <td>0.860</td>\n",
       "      <td>3.000</td>\n",
       "      <td>8.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.760</td>\n",
       "      <td>0.880</td>\n",
       "      <td>4.000</td>\n",
       "      <td>13.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.810</td>\n",
       "      <td>0.900</td>\n",
       "      <td>5.000</td>\n",
       "      <td>22.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.950</td>\n",
       "      <td>0.970</td>\n",
       "      <td>31.000</td>\n",
       "      <td>2357.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       cos_min  cos_mean  distinct_tier1s      Size\n",
       "count  280.000   280.000          280.000   280.000\n",
       "mean     0.755     0.879            4.636    25.850\n",
       "std      0.091     0.034            3.003   140.567\n",
       "min     -0.030     0.720            1.000     5.000\n",
       "25%      0.710     0.860            3.000     8.000\n",
       "50%      0.760     0.880            4.000    13.000\n",
       "75%      0.810     0.900            5.000    22.000\n",
       "max      0.950     0.970           31.000  2357.000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_info[['cos_min', 'cos_mean', 'distinct_tier1s', 'Size']].describe().apply(lambda x: round(x, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Topics are about:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine natural id inside docs_df\n",
    "\n",
    "docs_df = pd.merge(docs_df, train_df, how=\"left\", left_on=\"Doc\", right_on=\"clean_body\")\n",
    "docs_df.drop(\"clean_body\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Doc_ID</th>\n",
       "      <th>content_natid</th>\n",
       "      <th>tier1</th>\n",
       "      <th>topic_num</th>\n",
       "      <th>topic_words</th>\n",
       "      <th>Size</th>\n",
       "      <th>cos_min</th>\n",
       "      <th>cos_mean</th>\n",
       "      <th>distinct_tier1s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10 robot vacuums that clean so you dont have to. if youre like most people, vacuuming is low on your list of favorite chores. you can avoid cleaning, or enlist the aid of a robot vacuum to clean your home autonomously. the best robot vacuums are ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>blogandpostid/blog/post/4983-5ea87d15960ddf000681c604</td>\n",
       "      <td>Shopping</td>\n",
       "      <td>-1</td>\n",
       "      <td>coffee, age, china, government, leaders, women</td>\n",
       "      <td>2357</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.72</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                         Doc  \\\n",
       "0  10 robot vacuums that clean so you dont have to. if youre like most people, vacuuming is low on your list of favorite chores. you can avoid cleaning, or enlist the aid of a robot vacuum to clean your home autonomously. the best robot vacuums are ...   \n",
       "\n",
       "   Topic  Doc_ID                                          content_natid  \\\n",
       "0     -1       0  blogandpostid/blog/post/4983-5ea87d15960ddf000681c604   \n",
       "\n",
       "      tier1  topic_num                                     topic_words  Size  \\\n",
       "0  Shopping         -1  coffee, age, china, government, leaders, women  2357   \n",
       "\n",
       "   cos_min  cos_mean  distinct_tier1s  \n",
       "0    -0.03      0.72               31  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# contains natid, article body, assigned topic\n",
    "\n",
    "full_train_topics = pd.merge(docs_df, topic_info, how=\"left\", left_on=\"Topic\", right_on=\"topic_num\")\n",
    "full_train_topics.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Training Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(full_train_topics[full_train_topics.Topic==-1].shape[0]/full_train_topics.shape[0]*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opt-1 Prediction\n",
    "* same model predict on 7 days with repeating articles\n",
    "\n",
    "**Training data**\n",
    "\n",
    "* Date range 7 days -- Apr 26 (monday) till May 02 (sunday)\n",
    "\n",
    "* Eligible pool = articles with pvs threshold = 7,238 articles\n",
    "    * **Distinct topics including -1 for noise :: 328**\n",
    "\n",
    "**Prediction data**\n",
    "\n",
    "* Date range -- may 3 (monday) till May 09 (friday)\n",
    "* May 3 to May 9 repeating articles present\n",
    "* Eligible pool = articles with pvs > 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UMAP(dens_frac=0.0, dens_lambda=0.0, min_dist=0.0, n_neighbors=12,\n",
      "     random_state=42)\n",
      "\n",
      "HDBSCAN(min_samples=5, prediction_data=True)\n"
     ]
    }
   ],
   "source": [
    "print(umap_embeddings)\n",
    "print()\n",
    "print(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_num</th>\n",
       "      <th>topic_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>coffee, age, china, government, leaders, women</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>10 states, average annual, annual wage, salaries, occupational, therapist salary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>kentucky derby, essential quality, horses, churchill downs, race, triple crown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>million viewers, 000 viewers, fox news, msnbc, cable news, prime time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>packers, quarterback, round, rodgers, nfl draft, green bay</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   topic_num  \\\n",
       "0         -1   \n",
       "1          0   \n",
       "2          1   \n",
       "3          2   \n",
       "4          3   \n",
       "\n",
       "                                                                        topic_words  \n",
       "0                                    coffee, age, china, government, leaders, women  \n",
       "1  10 states, average annual, annual wage, salaries, occupational, therapist salary  \n",
       "2    kentucky derby, essential quality, horses, churchill downs, race, triple crown  \n",
       "3             million viewers, 000 viewers, fox news, msnbc, cable news, prime time  \n",
       "4                        packers, quarterback, round, rodgers, nfl draft, green bay  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_n_words_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function needs top_n_words_df defined above. so run above cells first\n",
    "\n",
    "def predict(umap_embeddings, cluster, embed_list, pred_df):\n",
    "    \n",
    "    '''\n",
    "    reusable predict function - \n",
    "    1. calculates predictions on incoming data and \n",
    "    2. clustering quality metrics like cossine similarirty min, mean, topic size etc\n",
    "    '''\n",
    "    \n",
    "    # dimensionality reduction\n",
    "    pred_umap_embeddings = umap_embeddings.transform(embed_list)\n",
    "\n",
    "    print(\"Training input shape:\", umap_embeddings.embedding_.shape)\n",
    "    print(\"Prediction input shape:\", pred_umap_embeddings.shape) #takes a min\n",
    "    print()\n",
    "    \n",
    "    # predict\n",
    "    pred_test_labels, pred_strengths = hdbscan.approximate_predict(cluster, pred_umap_embeddings)\n",
    "    \n",
    "    # put topics in dataframe\n",
    "    pred_docs_df = pred_df[['clean_body']]\n",
    "    pred_docs_df.rename(columns={'clean_body':'Doc'}, inplace=True)\n",
    "    pred_docs_df[\"Topic\"] = pred_test_labels\n",
    "\n",
    "    print(\"Distinct topics including -1 for noise\", len(pred_docs_df.Topic.unique()))\n",
    "    \n",
    "    # combine natural id inside docs_df\n",
    "\n",
    "    pred_docs_df = pd.merge(pred_docs_df, pred_df, how=\"left\", left_on=\"Doc\", right_on=\"clean_body\")\n",
    "    pred_docs_df.drop(\"clean_body\", axis=1, inplace=True)\n",
    "    \n",
    "    pred_full = pd.merge(pred_docs_df, \n",
    "         top_n_words_df, \n",
    "         how=\"left\", \n",
    "         left_on=\"Topic\", \n",
    "         right_on = \"topic_num\").drop('topic_num', axis=1)\n",
    "    \n",
    "    return pred_full, pred_umap_embeddings, pred_strengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# this function needs top_n_words_df\n",
    "# uses original article embeddings (not dimensionality reduced ones) to compute topic-embeddings &\n",
    "# to compute cos-sim-score b/w article and topic-embedding\n",
    "\n",
    "def cluster_noise(pred_df, art_embed):\n",
    "    \n",
    "    '''\n",
    "    reusable function which - \n",
    "    1. calculates an embedding representation of every trained topic  \n",
    "    2. each day, assigns noise point to closest matching topic-embedding with threshold >=0.93\n",
    "    '''\n",
    "    \n",
    "    nonnoise_df = pred_df[pred_df.Topic != -1].copy()\n",
    "    noise_df = pred_df[pred_df.Topic == -1].copy()\n",
    "    \n",
    "    print(\"Non-Noise DF: \", nonnoise_df.shape)\n",
    "    print(\"Noise DF: \", noise_df.shape)\n",
    "    \n",
    "    nonnoise_embeds = [art_embed[i] for i in list(nonnoise_df.index)] \n",
    "    noise_embeds = [art_embed[i] for i in list(noise_df.index)] \n",
    "\n",
    "    print(\"Non-Noise embed len: \", len(nonnoise_embeds))\n",
    "    print(\"Noise embed len: \", len(noise_embeds))\n",
    "\n",
    "    nonnoise_df.reset_index(drop=True, inplace=True)\n",
    "    noise_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    topic_embeddings = []\n",
    "\n",
    "    for topic_num in sorted(nonnoise_df.Topic.unique()):\n",
    "\n",
    "        topic_group = nonnoise_df[nonnoise_df.Topic == topic_num]\n",
    "\n",
    "        # grab all articles of given topic\n",
    "        group_embeddings = [nonnoise_embeds[i] for i in list(topic_group.index)]\n",
    "\n",
    "        # calculate average embedding for each topic\n",
    "        topic_embeddings.append(np.mean(group_embeddings, axis=0))\n",
    "\n",
    "    print(\"Total topics\", len(topic_embeddings))\n",
    "    \n",
    "    closest_topic = []\n",
    "    closest_cosine_score = []\n",
    "\n",
    "    for i in range(len(noise_embeds)):\n",
    "        \n",
    "        # insert the noise article at position 0 of list\n",
    "        topic_embeddings.insert(0, noise_embeds[i])\n",
    "\n",
    "        # convert list to sparse matrix\n",
    "        sparse_matrix= scipy.sparse.csr_matrix(topic_embeddings)\n",
    "\n",
    "        # full matrix of cosine similarity of noise article with every topic-embeddibng\n",
    "        cos_mat = cosine_similarity(sparse_matrix)\n",
    "        \n",
    "        # select the max cosine score of noise with topic\n",
    "        closest_cosine_score.append(sorted(cos_mat[0][1:])[-1])\n",
    "        \n",
    "        # if max cosine score passes threshold:\n",
    "        if sorted(cos_mat[0][1:])[-1] >=0.93:\n",
    "            # find closest topic\n",
    "            closest_topic.append(sorted(nonnoise_df.Topic.unique())[np.argmax(cos_mat[0][1:])])\n",
    "        else:\n",
    "            # keep as noise\n",
    "            closest_topic.append(-1)\n",
    "\n",
    "        # remove current noise article from position 0\n",
    "        topic_embeddings.pop(0) # 4mins\n",
    "        \n",
    "    noise_df[\"cos_closest_topic\"] = closest_topic\n",
    "    noise_df[\"cosine_sim_score\"] = closest_cosine_score\n",
    "\n",
    "    cossin_insp = pd.merge(noise_df[['content_natid', 'cos_closest_topic', 'cosine_sim_score']], \n",
    "                           top_n_words_df, \n",
    "                           how=\"left\", \n",
    "                           left_on=\"cos_closest_topic\", \n",
    "                           right_on=\"topic_num\")\n",
    "    \n",
    "    return cossin_insp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE - saving these topic embeddings dont make a whole lot of sense. \n",
    "# Recomputing topic embeddings every day just takes 4 - 5 mins as such"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_final_pred(pred_df, clust_noise_df):\n",
    "    \n",
    "    print(\"Pred df: \", pred_df.shape)\n",
    "    print(\"Clustered Noise df: \", clust_noise_df.shape)\n",
    "    \n",
    "    full_df = pd.merge(pred_df[['content_natid', 'Doc', 'Topic', 'topic_words', 'pvs', 'total_timeonpage', 'pub_date',\n",
    "                                'tier1', 'tier2', 'article_length']], \n",
    "                     clust_noise_df, \n",
    "                     on=\"content_natid\", \n",
    "                     how=\"left\")\n",
    "    \n",
    "    full_df.rename(columns={\n",
    "        'Topic': 'orig_pred_topic', \n",
    "        'topic_words_x': 'orig_topic_labels',\n",
    "        'cos_closest_topic': 'noise_closest_topic', \n",
    "        'topic_words_y': 'noise_topic_labels'}, \n",
    "                     inplace=True)\n",
    "    \n",
    "    full_df['Topic'] = full_df.noise_closest_topic.fillna(full_df.orig_pred_topic)\n",
    "    full_df['final_topic_labels'] = full_df.noise_topic_labels.fillna(full_df.orig_topic_labels)\n",
    "    \n",
    "    full_df.Topic = full_df.Topic.astype(int)\n",
    "    full_df.noise_closest_topic =full_df.noise_closest_topic.astype('Int64')\n",
    "    \n",
    "    pred_topic_sizes = extract_topic_sizes(full_df)\n",
    "    \n",
    "    # find the topics with size=1\n",
    "    size1_topics = pred_topic_sizes[pred_topic_sizes.Size==1].Topic.values\n",
    "    \n",
    "    # replace their topics to -1 i.e. assign those articles to noise\n",
    "    full_df.Topic = np.where(full_df.Topic.isin(size1_topics), \n",
    "                    -1,\n",
    "                    full_df.Topic)\n",
    "\n",
    "    return full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_topic_metrics(pred_docs_df, pred_umap_embeddings):\n",
    "    \n",
    "    # GENERATE TOPIC METRICS\n",
    "    pred_topic_sizes = extract_topic_sizes(pred_docs_df)\n",
    "\n",
    "    # per topic - cos_min and cos_mean\n",
    "    pred_cossim_df = generate_all_cossim_metrics(pred_docs_df, pred_umap_embeddings)\n",
    "\n",
    "    # per topic - tier counts\n",
    "    iab_metric = pred_docs_df.groupby(\"Topic\").agg({\"tier1\": \n",
    "                                                   'nunique'}).round(2).reset_index().rename(columns = {'tier1': \n",
    "                                                                                                        'distinct_tier1s',\n",
    "                                                                                                        'Topic':\n",
    "                                                                                                        'topic_num'})\n",
    "\n",
    "    # combine topic, top words, cos and tier metrics\n",
    "    topic_info = pd.merge(pd.merge(pred_topic_sizes, \n",
    "                                            pred_cossim_df, how=\"inner\", left_on=\"Topic\", right_on = \"topic_num\"), \n",
    "                                   iab_metric, on = \"topic_num\", how=\"inner\")\n",
    "\n",
    "    topic_info.drop('Topic', axis=1, inplace=True)\n",
    "\n",
    "    # combine topic_info with topic names\n",
    "\n",
    "    topic_info = pd.merge(topic_info, top_n_words_df, how=\"left\", on=\"topic_num\").sort_values(\"topic_num\")\n",
    "    \n",
    "    return topic_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Predict new points/articles - May 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training input shape: (7238, 2)\n",
      "Prediction input shape: (4199, 2)\n",
      "\n",
      "Distinct topics including -1 for noise 267\n",
      "\n",
      "time (mins):  0.29\n",
      "Noise before:\n",
      "(1920, 11)\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "pred_may03, may03_pred_embeds, may03_strengths = predict(umap_embeddings, cluster, \n",
    "                                                            may03_art_embed, may03) \n",
    "\n",
    "end = time.time()\n",
    "print()\n",
    "print(\"time (mins): \", round((end - start)/60, 2)) \n",
    "\n",
    "print(\"Noise before:\")\n",
    "print(pred_may03[pred_may03.Topic==-1].shape) # earlier 310 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-Noise DF:  (2279, 11)\n",
      "Noise DF:  (1920, 11)\n",
      "Non-Noise embed len:  2279\n",
      "Noise embed len:  1920\n",
      "Total topics 266\n",
      "Pred df:  (4199, 11)\n",
      "Clustered Noise df:  (1920, 4)\n",
      "\n",
      "time (mins):  3.16\n",
      "Noise After:\n",
      "(1011, 15)\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "may03_noise_clust = cluster_noise(pred_may03, may03_art_embed) \n",
    "may03_noise_clust.drop(\"topic_num\", axis=1, inplace=True)\n",
    "\n",
    "may03_full_pred = prep_final_pred(pred_may03, may03_noise_clust) \n",
    "\n",
    "end = time.time()\n",
    "print()\n",
    "print(\"time (mins): \", round((end - start)/60, 2)) \n",
    "\n",
    "print(\"Noise After:\")\n",
    "print(may03_full_pred[may03_full_pred.Topic==-1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cos_min</th>\n",
       "      <th>cos_mean</th>\n",
       "      <th>distinct_tier1s</th>\n",
       "      <th>Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>251.000</td>\n",
       "      <td>251.000</td>\n",
       "      <td>251.000</td>\n",
       "      <td>251.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.796</td>\n",
       "      <td>0.889</td>\n",
       "      <td>3.793</td>\n",
       "      <td>12.701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.080</td>\n",
       "      <td>0.038</td>\n",
       "      <td>2.208</td>\n",
       "      <td>9.765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.560</td>\n",
       "      <td>0.740</td>\n",
       "      <td>1.000</td>\n",
       "      <td>2.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.750</td>\n",
       "      <td>0.870</td>\n",
       "      <td>2.000</td>\n",
       "      <td>6.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.800</td>\n",
       "      <td>0.900</td>\n",
       "      <td>3.000</td>\n",
       "      <td>11.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.850</td>\n",
       "      <td>0.910</td>\n",
       "      <td>5.000</td>\n",
       "      <td>16.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.970</td>\n",
       "      <td>0.970</td>\n",
       "      <td>14.000</td>\n",
       "      <td>58.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       cos_min  cos_mean  distinct_tier1s     Size\n",
       "count  251.000   251.000          251.000  251.000\n",
       "mean     0.796     0.889            3.793   12.701\n",
       "std      0.080     0.038            2.208    9.765\n",
       "min      0.560     0.740            1.000    2.000\n",
       "25%      0.750     0.870            2.000    6.000\n",
       "50%      0.800     0.900            3.000   11.000\n",
       "75%      0.850     0.910            5.000   16.500\n",
       "max      0.970     0.970           14.000   58.000"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "may3_topic_info = generate_topic_metrics(may03_full_pred, may03_art_embed)\n",
    "may3_topic_info[['cos_min', 'cos_mean', 'distinct_tier1s', 'Size']][1:].describe().apply(lambda x: round(x, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "may03_full_pred.to_csv(\"may03_final_eval.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pvs\n",
       "(100, 1000]        1734\n",
       "(1000, 5000]        148\n",
       "(5000, 10000]        26\n",
       "(10000, 50000]        8\n",
       "(50000, 100000]       1\n",
       "Name: content_natid, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only 9 high pageview articles (pv>=10k) lost to noise on May 3\n",
    "\n",
    "pvs_dist = pd.merge(may03_noise_clust, \n",
    "                    may03[['content_natid', 'pvs']], \n",
    "                    on=\"content_natid\", how=\"left\")\n",
    "\n",
    "ranges = [100,1000,5000,10000, 50000,100000]\n",
    "pvs_dist['content_natid'].groupby(pd.cut(pvs_dist.pvs, ranges)).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Predict new points/articles - May 04**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training input shape: (7238, 2)\n",
      "Prediction input shape: (4223, 2)\n",
      "\n",
      "Distinct topics including -1 for noise 271\n",
      "\n",
      "time (mins):  0.09\n",
      "Noise before:\n",
      "(2000, 11)\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "pred_may04, may04_pred_embeds, may04_strengths = predict(umap_embeddings, cluster, \n",
    "                                                            may04_art_embed, may04) \n",
    "\n",
    "end = time.time()\n",
    "print()\n",
    "print(\"time (mins): \", round((end - start)/60, 2)) \n",
    "\n",
    "print(\"Noise before:\")\n",
    "print(pred_may04[pred_may04.Topic==-1].shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-Noise DF:  (2223, 11)\n",
      "Noise DF:  (2000, 11)\n",
      "Non-Noise embed len:  2223\n",
      "Noise embed len:  2000\n",
      "Total topics 270\n",
      "Pred df:  (4223, 11)\n",
      "Clustered Noise df:  (2000, 4)\n",
      "\n",
      "time (mins):  3.38\n",
      "Noise After:\n",
      "(1035, 15)\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "may04_noise_clust = cluster_noise(pred_may04, may04_art_embed) \n",
    "may04_noise_clust.drop(\"topic_num\", axis=1, inplace=True)\n",
    "\n",
    "may04_full_pred = prep_final_pred(pred_may04, may04_noise_clust) \n",
    "\n",
    "end = time.time()\n",
    "print()\n",
    "print(\"time (mins): \", round((end - start)/60, 2))\n",
    "\n",
    "print(\"Noise After:\")\n",
    "print(may04_full_pred[may04_full_pred.Topic==-1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cos_min</th>\n",
       "      <th>cos_mean</th>\n",
       "      <th>distinct_tier1s</th>\n",
       "      <th>Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>254.000</td>\n",
       "      <td>254.000</td>\n",
       "      <td>254.000</td>\n",
       "      <td>254.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.796</td>\n",
       "      <td>0.888</td>\n",
       "      <td>3.854</td>\n",
       "      <td>12.551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.100</td>\n",
       "      <td>0.043</td>\n",
       "      <td>2.197</td>\n",
       "      <td>9.880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.340</td>\n",
       "      <td>0.690</td>\n",
       "      <td>1.000</td>\n",
       "      <td>2.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.750</td>\n",
       "      <td>0.870</td>\n",
       "      <td>2.000</td>\n",
       "      <td>5.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.810</td>\n",
       "      <td>0.895</td>\n",
       "      <td>3.000</td>\n",
       "      <td>10.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.860</td>\n",
       "      <td>0.920</td>\n",
       "      <td>5.000</td>\n",
       "      <td>17.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.970</td>\n",
       "      <td>0.970</td>\n",
       "      <td>14.000</td>\n",
       "      <td>52.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       cos_min  cos_mean  distinct_tier1s     Size\n",
       "count  254.000   254.000          254.000  254.000\n",
       "mean     0.796     0.888            3.854   12.551\n",
       "std      0.100     0.043            2.197    9.880\n",
       "min      0.340     0.690            1.000    2.000\n",
       "25%      0.750     0.870            2.000    5.000\n",
       "50%      0.810     0.895            3.000   10.000\n",
       "75%      0.860     0.920            5.000   17.000\n",
       "max      0.970     0.970           14.000   52.000"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "may4_topic_info = generate_topic_metrics(may04_full_pred, may04_art_embed)\n",
    "may4_topic_info[['cos_min', 'cos_mean', 'distinct_tier1s', 'Size']][1:].describe().apply(lambda x: round(x, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "may04_full_pred.to_csv(\"may04_final_eval.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Predict new points/articles - May 05**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training input shape: (7238, 2)\n",
      "Prediction input shape: (4160, 2)\n",
      "\n",
      "Distinct topics including -1 for noise 267\n",
      "\n",
      "time (mins):  0.07\n",
      "Noise before:\n",
      "(1912, 11)\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "pred_may05, may05_pred_embeds, may05_strengths = predict(umap_embeddings, cluster, \n",
    "                                                            may05_art_embed, may05) \n",
    "\n",
    "end = time.time()\n",
    "print()\n",
    "print(\"time (mins): \", round((end - start)/60, 2)) \n",
    "\n",
    "print(\"Noise before:\")\n",
    "print(pred_may05[pred_may05.Topic==-1].shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-Noise DF:  (2248, 11)\n",
      "Noise DF:  (1912, 11)\n",
      "Non-Noise embed len:  2248\n",
      "Noise embed len:  1912\n",
      "Total topics 266\n",
      "Pred df:  (4160, 11)\n",
      "Clustered Noise df:  (1912, 4)\n",
      "\n",
      "time (mins):  3.16\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "may05_noise_clust = cluster_noise(pred_may05, may05_art_embed) \n",
    "may05_noise_clust.drop(\"topic_num\", axis=1, inplace=True)\n",
    "\n",
    "may05_full_pred = prep_final_pred(pred_may05, may05_noise_clust) \n",
    "\n",
    "end = time.time()\n",
    "print()\n",
    "print(\"time (mins): \", round((end - start)/60, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noise After:\n",
      "(990, 15)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cos_min</th>\n",
       "      <th>cos_mean</th>\n",
       "      <th>distinct_tier1s</th>\n",
       "      <th>Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>254.000</td>\n",
       "      <td>254.000</td>\n",
       "      <td>254.000</td>\n",
       "      <td>254.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.797</td>\n",
       "      <td>0.888</td>\n",
       "      <td>3.843</td>\n",
       "      <td>12.480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.088</td>\n",
       "      <td>0.043</td>\n",
       "      <td>2.128</td>\n",
       "      <td>9.322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.430</td>\n",
       "      <td>0.600</td>\n",
       "      <td>1.000</td>\n",
       "      <td>2.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.750</td>\n",
       "      <td>0.870</td>\n",
       "      <td>2.000</td>\n",
       "      <td>6.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.805</td>\n",
       "      <td>0.890</td>\n",
       "      <td>3.000</td>\n",
       "      <td>10.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.860</td>\n",
       "      <td>0.910</td>\n",
       "      <td>5.000</td>\n",
       "      <td>16.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.970</td>\n",
       "      <td>0.980</td>\n",
       "      <td>13.000</td>\n",
       "      <td>50.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       cos_min  cos_mean  distinct_tier1s     Size\n",
       "count  254.000   254.000          254.000  254.000\n",
       "mean     0.797     0.888            3.843   12.480\n",
       "std      0.088     0.043            2.128    9.322\n",
       "min      0.430     0.600            1.000    2.000\n",
       "25%      0.750     0.870            2.000    6.000\n",
       "50%      0.805     0.890            3.000   10.000\n",
       "75%      0.860     0.910            5.000   16.750\n",
       "max      0.970     0.980           13.000   50.000"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Noise After:\")\n",
    "print(may05_full_pred[may05_full_pred.Topic==-1].shape)\n",
    "\n",
    "may5_topic_info = generate_topic_metrics(may05_full_pred, may05_art_embed)\n",
    "may5_topic_info[['cos_min', 'cos_mean', 'distinct_tier1s', 'Size']][1:].describe().apply(lambda x: round(x, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "may05_full_pred.to_csv(\"may05_final_eval.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Predict new points/articles - May 06**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training input shape: (7238, 2)\n",
      "Prediction input shape: (4063, 2)\n",
      "\n",
      "Distinct topics including -1 for noise 264\n",
      "\n",
      "time (mins):  0.09\n",
      "Noise before:\n",
      "(1923, 11)\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "pred_may06, may06_pred_embeds, may06_strengths = predict(umap_embeddings, cluster, \n",
    "                                                            may06_art_embed, may06) \n",
    "\n",
    "end = time.time()\n",
    "print()\n",
    "print(\"time (mins): \", round((end - start)/60, 2)) \n",
    "\n",
    "print(\"Noise before:\")\n",
    "print(pred_may06[pred_may06.Topic==-1].shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-Noise DF:  (2140, 11)\n",
      "Noise DF:  (1923, 11)\n",
      "Non-Noise embed len:  2140\n",
      "Noise embed len:  1923\n",
      "Total topics 263\n",
      "Pred df:  (4063, 11)\n",
      "Clustered Noise df:  (1923, 4)\n",
      "\n",
      "time (mins):  3.08\n",
      "Noise After:\n",
      "(1010, 15)\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "may06_noise_clust = cluster_noise(pred_may06, may06_art_embed) \n",
    "may06_noise_clust.drop(\"topic_num\", axis=1, inplace=True)\n",
    "\n",
    "may06_full_pred = prep_final_pred(pred_may06, may06_noise_clust) \n",
    "\n",
    "end = time.time()\n",
    "print()\n",
    "print(\"time (mins): \", round((end - start)/60, 2))\n",
    "\n",
    "print(\"Noise After:\")\n",
    "print(may06_full_pred[may06_full_pred.Topic==-1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cos_min</th>\n",
       "      <th>cos_mean</th>\n",
       "      <th>distinct_tier1s</th>\n",
       "      <th>Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>254.000</td>\n",
       "      <td>254.000</td>\n",
       "      <td>254.000</td>\n",
       "      <td>254.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.793</td>\n",
       "      <td>0.887</td>\n",
       "      <td>3.815</td>\n",
       "      <td>12.020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.092</td>\n",
       "      <td>0.041</td>\n",
       "      <td>2.225</td>\n",
       "      <td>9.022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.370</td>\n",
       "      <td>0.730</td>\n",
       "      <td>1.000</td>\n",
       "      <td>2.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.750</td>\n",
       "      <td>0.870</td>\n",
       "      <td>2.000</td>\n",
       "      <td>5.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.810</td>\n",
       "      <td>0.900</td>\n",
       "      <td>3.000</td>\n",
       "      <td>10.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.860</td>\n",
       "      <td>0.910</td>\n",
       "      <td>5.000</td>\n",
       "      <td>16.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.970</td>\n",
       "      <td>0.980</td>\n",
       "      <td>13.000</td>\n",
       "      <td>56.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       cos_min  cos_mean  distinct_tier1s     Size\n",
       "count  254.000   254.000          254.000  254.000\n",
       "mean     0.793     0.887            3.815   12.020\n",
       "std      0.092     0.041            2.225    9.022\n",
       "min      0.370     0.730            1.000    2.000\n",
       "25%      0.750     0.870            2.000    5.000\n",
       "50%      0.810     0.900            3.000   10.000\n",
       "75%      0.860     0.910            5.000   16.000\n",
       "max      0.970     0.980           13.000   56.000"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "may6_topic_info = generate_topic_metrics(may06_full_pred, may06_art_embed)\n",
    "may6_topic_info[['cos_min', 'cos_mean', 'distinct_tier1s', 'Size']][1:].describe().apply(lambda x: round(x, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "may06_full_pred.to_csv(\"may06_final_eval.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Predict new points/articles - May 07**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training input shape: (7238, 2)\n",
      "Prediction input shape: (3630, 2)\n",
      "\n",
      "Distinct topics including -1 for noise 259\n",
      "\n",
      "time (mins):  0.08\n",
      "Noise before:\n",
      "(1740, 11)\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "pred_may07, may07_pred_embeds, may07_strengths = predict(umap_embeddings, cluster, \n",
    "                                                            may07_art_embed, may07) \n",
    "\n",
    "end = time.time()\n",
    "print()\n",
    "print(\"time (mins): \", round((end - start)/60, 2)) \n",
    "\n",
    "print(\"Noise before:\")\n",
    "print(pred_may07[pred_may07.Topic==-1].shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-Noise DF:  (1890, 11)\n",
      "Noise DF:  (1740, 11)\n",
      "Non-Noise embed len:  1890\n",
      "Noise embed len:  1740\n",
      "Total topics 258\n",
      "Pred df:  (3630, 11)\n",
      "Clustered Noise df:  (1740, 4)\n",
      "\n",
      "time (mins):  2.7\n",
      "Noise After:\n",
      "(929, 15)\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "may07_noise_clust = cluster_noise(pred_may07, may07_art_embed) \n",
    "may07_noise_clust.drop(\"topic_num\", axis=1, inplace=True)\n",
    "\n",
    "may07_full_pred = prep_final_pred(pred_may07, may07_noise_clust) \n",
    "\n",
    "end = time.time()\n",
    "print()\n",
    "print(\"time (mins): \", round((end - start)/60, 2))\n",
    "\n",
    "print(\"Noise After:\")\n",
    "print(may07_full_pred[may07_full_pred.Topic==-1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cos_min</th>\n",
       "      <th>cos_mean</th>\n",
       "      <th>distinct_tier1s</th>\n",
       "      <th>Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>245.000</td>\n",
       "      <td>245.000</td>\n",
       "      <td>245.000</td>\n",
       "      <td>245.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.795</td>\n",
       "      <td>0.885</td>\n",
       "      <td>3.604</td>\n",
       "      <td>11.024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.097</td>\n",
       "      <td>0.045</td>\n",
       "      <td>2.019</td>\n",
       "      <td>8.558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.310</td>\n",
       "      <td>0.670</td>\n",
       "      <td>1.000</td>\n",
       "      <td>2.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.760</td>\n",
       "      <td>0.870</td>\n",
       "      <td>2.000</td>\n",
       "      <td>5.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.810</td>\n",
       "      <td>0.900</td>\n",
       "      <td>3.000</td>\n",
       "      <td>8.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.860</td>\n",
       "      <td>0.910</td>\n",
       "      <td>4.000</td>\n",
       "      <td>14.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.970</td>\n",
       "      <td>0.980</td>\n",
       "      <td>13.000</td>\n",
       "      <td>43.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       cos_min  cos_mean  distinct_tier1s     Size\n",
       "count  245.000   245.000          245.000  245.000\n",
       "mean     0.795     0.885            3.604   11.024\n",
       "std      0.097     0.045            2.019    8.558\n",
       "min      0.310     0.670            1.000    2.000\n",
       "25%      0.760     0.870            2.000    5.000\n",
       "50%      0.810     0.900            3.000    8.000\n",
       "75%      0.860     0.910            4.000   14.000\n",
       "max      0.970     0.980           13.000   43.000"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "may7_topic_info = generate_topic_metrics(may07_full_pred, may07_art_embed)\n",
    "may7_topic_info[['cos_min', 'cos_mean', 'distinct_tier1s', 'Size']][1:].describe().apply(lambda x: round(x, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "may07_full_pred.to_csv(\"may07_final_eval.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Predict new points/articles - May 08**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training input shape: (7238, 2)\n",
      "Prediction input shape: (3025, 2)\n",
      "\n",
      "Distinct topics including -1 for noise 247\n",
      "\n",
      "time (mins):  0.07\n",
      "Noise before:\n",
      "(1414, 11)\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "pred_may08, may08_pred_embeds, may08_strengths = predict(umap_embeddings, cluster, \n",
    "                                                            may08_art_embed, may08) \n",
    "\n",
    "end = time.time()\n",
    "print()\n",
    "print(\"time (mins): \", round((end - start)/60, 2)) \n",
    "\n",
    "print(\"Noise before:\")\n",
    "print(pred_may08[pred_may08.Topic==-1].shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-Noise DF:  (1611, 11)\n",
      "Noise DF:  (1414, 11)\n",
      "Non-Noise embed len:  1611\n",
      "Noise embed len:  1414\n",
      "Total topics 246\n",
      "Pred df:  (3025, 11)\n",
      "Clustered Noise df:  (1414, 4)\n",
      "\n",
      "time (mins):  2.01\n",
      "Noise After:\n",
      "(832, 15)\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "may08_noise_clust = cluster_noise(pred_may08, may08_art_embed) \n",
    "may08_noise_clust.drop(\"topic_num\", axis=1, inplace=True)\n",
    "\n",
    "may08_full_pred = prep_final_pred(pred_may08, may08_noise_clust) \n",
    "\n",
    "end = time.time()\n",
    "print()\n",
    "print(\"time (mins): \", round((end - start)/60, 2))\n",
    "\n",
    "print(\"Noise After:\")\n",
    "print(may08_full_pred[may08_full_pred.Topic==-1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cos_min</th>\n",
       "      <th>cos_mean</th>\n",
       "      <th>distinct_tier1s</th>\n",
       "      <th>Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>221.000</td>\n",
       "      <td>221.000</td>\n",
       "      <td>221.000</td>\n",
       "      <td>221.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.803</td>\n",
       "      <td>0.886</td>\n",
       "      <td>3.498</td>\n",
       "      <td>9.923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.092</td>\n",
       "      <td>0.043</td>\n",
       "      <td>2.006</td>\n",
       "      <td>7.530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.390</td>\n",
       "      <td>0.660</td>\n",
       "      <td>1.000</td>\n",
       "      <td>2.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.770</td>\n",
       "      <td>0.860</td>\n",
       "      <td>2.000</td>\n",
       "      <td>4.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.810</td>\n",
       "      <td>0.890</td>\n",
       "      <td>3.000</td>\n",
       "      <td>7.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.870</td>\n",
       "      <td>0.920</td>\n",
       "      <td>5.000</td>\n",
       "      <td>14.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.970</td>\n",
       "      <td>0.970</td>\n",
       "      <td>13.000</td>\n",
       "      <td>41.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       cos_min  cos_mean  distinct_tier1s     Size\n",
       "count  221.000   221.000          221.000  221.000\n",
       "mean     0.803     0.886            3.498    9.923\n",
       "std      0.092     0.043            2.006    7.530\n",
       "min      0.390     0.660            1.000    2.000\n",
       "25%      0.770     0.860            2.000    4.000\n",
       "50%      0.810     0.890            3.000    7.000\n",
       "75%      0.870     0.920            5.000   14.000\n",
       "max      0.970     0.970           13.000   41.000"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "may8_topic_info = generate_topic_metrics(may08_full_pred, may08_art_embed)\n",
    "may8_topic_info[['cos_min', 'cos_mean', 'distinct_tier1s', 'Size']][1:].describe().apply(lambda x: round(x, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "may08_full_pred.to_csv(\"may08_final_eval.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Predict new points/articles - May 09**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training input shape: (7238, 2)\n",
      "Prediction input shape: (3231, 2)\n",
      "\n",
      "Distinct topics including -1 for noise 254\n",
      "\n",
      "time (mins):  0.07\n",
      "Noise before:\n",
      "(1514, 11)\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "pred_may09, may09_pred_embeds, may09_strengths = predict(umap_embeddings, cluster, \n",
    "                                                            may09_art_embed, may09) \n",
    "\n",
    "end = time.time()\n",
    "print()\n",
    "print(\"time (mins): \", round((end - start)/60, 2)) \n",
    "\n",
    "print(\"Noise before:\")\n",
    "print(pred_may09[pred_may09.Topic==-1].shape) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-Noise DF:  (1717, 11)\n",
      "Noise DF:  (1514, 11)\n",
      "Non-Noise embed len:  1717\n",
      "Noise embed len:  1514\n",
      "Total topics 253\n",
      "Pred df:  (3231, 11)\n",
      "Clustered Noise df:  (1514, 4)\n",
      "\n",
      "time (mins):  2.27\n",
      "Noise After:\n",
      "(856, 15)\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "may09_noise_clust = cluster_noise(pred_may09, may09_art_embed) \n",
    "may09_noise_clust.drop(\"topic_num\", axis=1, inplace=True)\n",
    "\n",
    "may09_full_pred = prep_final_pred(pred_may09, may09_noise_clust) \n",
    "\n",
    "end = time.time()\n",
    "print()\n",
    "print(\"time (mins): \", round((end - start)/60, 2))\n",
    "\n",
    "print(\"Noise After:\")\n",
    "print(may09_full_pred[may09_full_pred.Topic==-1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cos_min</th>\n",
       "      <th>cos_mean</th>\n",
       "      <th>distinct_tier1s</th>\n",
       "      <th>Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>228.000</td>\n",
       "      <td>228.000</td>\n",
       "      <td>228.000</td>\n",
       "      <td>228.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.804</td>\n",
       "      <td>0.886</td>\n",
       "      <td>3.482</td>\n",
       "      <td>10.417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.090</td>\n",
       "      <td>0.048</td>\n",
       "      <td>1.941</td>\n",
       "      <td>8.233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.380</td>\n",
       "      <td>0.630</td>\n",
       "      <td>1.000</td>\n",
       "      <td>2.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.760</td>\n",
       "      <td>0.870</td>\n",
       "      <td>2.000</td>\n",
       "      <td>4.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.810</td>\n",
       "      <td>0.900</td>\n",
       "      <td>3.000</td>\n",
       "      <td>8.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.870</td>\n",
       "      <td>0.920</td>\n",
       "      <td>5.000</td>\n",
       "      <td>14.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.970</td>\n",
       "      <td>0.970</td>\n",
       "      <td>12.000</td>\n",
       "      <td>46.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       cos_min  cos_mean  distinct_tier1s     Size\n",
       "count  228.000   228.000          228.000  228.000\n",
       "mean     0.804     0.886            3.482   10.417\n",
       "std      0.090     0.048            1.941    8.233\n",
       "min      0.380     0.630            1.000    2.000\n",
       "25%      0.760     0.870            2.000    4.000\n",
       "50%      0.810     0.900            3.000    8.000\n",
       "75%      0.870     0.920            5.000   14.000\n",
       "max      0.970     0.970           12.000   46.000"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "may9_topic_info = generate_topic_metrics(may09_full_pred, may09_art_embed)\n",
    "may9_topic_info[['cos_min', 'cos_mean', 'distinct_tier1s', 'Size']][1:].describe().apply(lambda x: round(x, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "may09_full_pred.to_csv(\"may09_final_eval.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- EDA ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eligible articles:\n",
      "May 3 - (4199, 15)\n",
      "May 4 - (4223, 15)\n",
      "May 5 - (4160, 15)\n",
      "May 6 - (4063, 15)\n",
      "May 7 - (3630, 15)\n",
      "May 8 - (3025, 15)\n",
      "May 9 - (3231, 15)\n"
     ]
    }
   ],
   "source": [
    "print(\"Eligible articles:\")\n",
    "print(\"May 3 -\", may03_full_pred.shape)\n",
    "print(\"May 4 -\", may04_full_pred.shape)\n",
    "print(\"May 5 -\", may05_full_pred.shape)\n",
    "print(\"May 6 -\", may06_full_pred.shape)\n",
    "print(\"May 7 -\", may07_full_pred.shape)\n",
    "print(\"May 8 -\", may08_full_pred.shape)\n",
    "print(\"May 9 -\", may09_full_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Topics assigned including noise:\n",
      "May 3 - 252\n",
      "May 4 - 255\n",
      "May 5 - 255\n",
      "May 6 - 255\n",
      "May 7 - 246\n",
      "May 8 - 222\n",
      "May 9 - 229\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Topics assigned including noise:\")\n",
    "print(\"May 3 -\",len(may03_full_pred.Topic.unique())) # 15 topics less\n",
    "print(\"May 4 -\",len(may04_full_pred.Topic.unique())) # 16 topics less\n",
    "print(\"May 5 -\",len(may05_full_pred.Topic.unique())) # 12 topics less\n",
    "print(\"May 6 -\",len(may06_full_pred.Topic.unique())) # 9 topics leass\n",
    "print(\"May 7 -\",len(may07_full_pred.Topic.unique())) # 13 topics less\n",
    "print(\"May 8 -\",len(may08_full_pred.Topic.unique())) # 25 topics less\n",
    "print(\"May 9 -\",len(may09_full_pred.Topic.unique())) # 25 topics less"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg topic size:\n",
      "May 3 - 17\n",
      "May 4 - 17\n",
      "May 5 - 16\n",
      "May 6 - 16\n",
      "May 7 - 15\n",
      "May 8 - 14\n",
      "May 9 - 14\n"
     ]
    }
   ],
   "source": [
    "print(\"Avg topic size:\")\n",
    "print(\"May 3 -\",round(may3_topic_info.Size.mean()))\n",
    "print(\"May 4 -\",round(may4_topic_info.Size.mean()))\n",
    "print(\"May 5 -\",round(may5_topic_info.Size.mean()))\n",
    "print(\"May 6 -\",round(may6_topic_info.Size.mean()))\n",
    "print(\"May 7 -\",round(may7_topic_info.Size.mean()))\n",
    "print(\"May 8 -\",round(may8_topic_info.Size.mean()))\n",
    "print(\"May 9 -\",round(may9_topic_info.Size.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = list(sorted(may03_full_pred.Topic.unique()))\n",
    "b = list(sorted(may04_full_pred.Topic.unique()))\n",
    "c = list(sorted(may05_full_pred.Topic.unique()))\n",
    "d = list(sorted(may06_full_pred.Topic.unique()))\n",
    "e = list(sorted(may07_full_pred.Topic.unique()))\n",
    "f = list(sorted(may08_full_pred.Topic.unique()))\n",
    "g = list(sorted(may09_full_pred.Topic.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "214"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# common topics - OLD\n",
    "\n",
    "elements_in_all = list(set.intersection(*map(set, [a, b, c, d, e, f, g])))\n",
    "len(elements_in_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "183"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# common topics\n",
    "\n",
    "elements_in_all = list(set.intersection(*map(set, [a, b, c, d, e, f, g])))\n",
    "len(elements_in_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of articles NOISE - \n",
      "May 3 - 1011\n",
      "May 4 - 1035\n",
      "May 5 - 990\n",
      "May 6 - 1010\n",
      "May 7 - 929\n",
      "May 8 - 832\n",
      "May 9 - 856\n"
     ]
    }
   ],
   "source": [
    "print(\"# of articles NOISE - \")\n",
    "print(\"May 3 -\",round(may03_full_pred[may03_full_pred.Topic==-1].shape[0]))\n",
    "print(\"May 4 -\",round(may04_full_pred[may04_full_pred.Topic==-1].shape[0]))\n",
    "print(\"May 5 -\",round(may05_full_pred[may05_full_pred.Topic==-1].shape[0]))\n",
    "print(\"May 6 -\",round(may06_full_pred[may06_full_pred.Topic==-1].shape[0]))\n",
    "print(\"May 7 -\",round(may07_full_pred[may07_full_pred.Topic==-1].shape[0]))\n",
    "print(\"May 8 -\",round(may08_full_pred[may08_full_pred.Topic==-1].shape[0]))\n",
    "print(\"May 9 -\",round(may09_full_pred[may09_full_pred.Topic==-1].shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perc of articles NOISE - \n",
      "May 3 - 24\n",
      "May 4 - 25\n",
      "May 5 - 24\n",
      "May 6 - 25\n",
      "May 7 - 26\n",
      "May 8 - 28\n",
      "May 9 - 26\n"
     ]
    }
   ],
   "source": [
    "print(\"Perc of articles NOISE - \")\n",
    "print(\"May 3 -\",round(may03_full_pred[may03_full_pred.Topic==-1].shape[0]/may03_full_pred.shape[0]*100))\n",
    "print(\"May 4 -\",round(may04_full_pred[may04_full_pred.Topic==-1].shape[0]/may04_full_pred.shape[0]*100))\n",
    "print(\"May 5 -\",round(may05_full_pred[may05_full_pred.Topic==-1].shape[0]/may05_full_pred.shape[0]*100))\n",
    "print(\"May 6 -\",round(may06_full_pred[may06_full_pred.Topic==-1].shape[0]/may06_full_pred.shape[0]*100))\n",
    "print(\"May 7 -\",round(may07_full_pred[may07_full_pred.Topic==-1].shape[0]/may07_full_pred.shape[0]*100))\n",
    "print(\"May 8 -\",round(may08_full_pred[may08_full_pred.Topic==-1].shape[0]/may08_full_pred.shape[0]*100))\n",
    "print(\"May 9 -\",round(may09_full_pred[may09_full_pred.Topic==-1].shape[0]/may09_full_pred.shape[0]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save in BQ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bq_cols = ['content_natid', 'Topic', 'final_topic_labels']\n",
    "bq_may3 = may03_full_pred[bq_cols]\n",
    "bq_may4 = may03_full_pred[bq_cols]\n",
    "bq_may5 = may03_full_pred[bq_cols]\n",
    "bq_may6 = may03_full_pred[bq_cols]\n",
    "bq_may7 = may03_full_pred[bq_cols]\n",
    "bq_may8 = may03_full_pred[bq_cols]\n",
    "bq_may3 = may03_full_pred[bq_cols]\n",
    "9\n",
    "print(\"May 4 -\", may04_full_pred.shape)\n",
    "print(\"May 5 -\", may05_full_pred.shape)\n",
    "print(\"May 6 -\", may06_full_pred.shape)\n",
    "print(\"May 7 -\", may07_full_pred.shape)\n",
    "print(\"May 8 -\", may08_full_pred.shape)\n",
    "print(\"May 9 -\", may09_full_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C-level vs Non-C-level**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Google imports\"\"\"\n",
    "from google.cloud import bigquery\n",
    "\n",
    "client = bigquery.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:  (5000000, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Non-DM    3236480\n",
       "DM        1763520\n",
       "Name: managementLevel, dtype: int64"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql = \"\"\"\n",
    "   SELECT \n",
    "       * \n",
    "   FROM \n",
    "       `api-project-901373404215.lookalike.zoom_info_dm` \n",
    "   LIMIT 5000000\n",
    "   \"\"\"\n",
    "#  WHERE date BETWEEN \"2021-05-03\" and \"2021-05-09\"\n",
    "\n",
    "# Send the query to the api and return a df\n",
    "zi_preds = client.query(sql).to_dataframe()\n",
    "print(\"Shape: \", zi_preds.shape)\n",
    "\n",
    "zi_preds.managementLevel.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:  (37768809, 2)\n"
     ]
    }
   ],
   "source": [
    "sql = \"\"\"\n",
    "   SELECT\n",
    "     GA_fullVisitorId, GA_cmsNaturalId\n",
    "    FROM\n",
    "      `api-project-901373404215.DataMart.v_DataMart_updated` \n",
    "   WHERE \n",
    "       GA_date BETWEEN \"2021-05-03\" and \"2021-05-09\"\n",
    "   \"\"\"\n",
    "\n",
    "# Send the query to the api and return a df\n",
    "ga = client.query(sql).to_dataframe()\n",
    "print(\"Shape: \", ga.shape) #6.44"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25169881"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 25MM unique fullvid in that 1 week\n",
    "len(ga.GA_fullVisitorId.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(984327, 2)\n",
      "368856\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GA_fullVisitorId</th>\n",
       "      <th>GA_cmsNaturalId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>9999961073753524873</td>\n",
       "      <td>blogandpostid/blog/post/50769-60934a675c40b40006893b41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>9997538034193996163</td>\n",
       "      <td>blogandpostid/blog/post/4773-605b795bafd8e40006c32a28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       GA_fullVisitorId  \\\n",
       "29  9999961073753524873   \n",
       "38  9997538034193996163   \n",
       "\n",
       "                                           GA_cmsNaturalId  \n",
       "29  blogandpostid/blog/post/50769-60934a675c40b40006893b41  \n",
       "38   blogandpostid/blog/post/4773-605b795bafd8e40006c32a28  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset = ga[ga.GA_fullVisitorId.isin(zi_preds.GA_fullVisitorId)]\n",
    "\n",
    "print(subset.shape)\n",
    "print(len(subset.GA_fullVisitorId.unique()))\n",
    "subset.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Non-DM    254503\n",
       "DM        118603\n",
       "Name: managementLevel, dtype: int64"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zi_nats = pd.merge(subset, zi_preds[['GA_fullVisitorId', 'managementLevel']], on=\"GA_fullVisitorId\", how=\"left\")\n",
    "\n",
    "# zi_nats = pd.merge(subset, zi_preds[['client', 'managementLevel']], left_on=\"GA_fullVisitorId\", \n",
    "#                    right_on = 'client', how=\"left\")\n",
    "\n",
    "zi_nats[['GA_fullVisitorId', 'managementLevel']].drop_duplicates(keep='first').managementLevel.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1223385, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GA_fullVisitorId</th>\n",
       "      <th>GA_cmsNaturalId</th>\n",
       "      <th>managementLevel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9999961073753524873</td>\n",
       "      <td>blogandpostid/blog/post/50769-60934a675c40b40006893b41</td>\n",
       "      <td>DM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9997538034193996163</td>\n",
       "      <td>blogandpostid/blog/post/4773-605b795bafd8e40006c32a28</td>\n",
       "      <td>DM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9995014657333051717</td>\n",
       "      <td>blogandpostid/blog/post/1360-6064d9c005bb9a0006238fbd</td>\n",
       "      <td>Non-DM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9999615644474572026</td>\n",
       "      <td>blogandpostid/blog/post/1383-608ca73a171509000659ce03</td>\n",
       "      <td>DM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9994983057241825546</td>\n",
       "      <td>blogandpostid/blog/post/1016-15628</td>\n",
       "      <td>Non-DM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      GA_fullVisitorId  \\\n",
       "0  9999961073753524873   \n",
       "1  9997538034193996163   \n",
       "2  9995014657333051717   \n",
       "3  9999615644474572026   \n",
       "4  9994983057241825546   \n",
       "\n",
       "                                          GA_cmsNaturalId managementLevel  \n",
       "0  blogandpostid/blog/post/50769-60934a675c40b40006893b41              DM  \n",
       "1   blogandpostid/blog/post/4773-605b795bafd8e40006c32a28              DM  \n",
       "2   blogandpostid/blog/post/1360-6064d9c005bb9a0006238fbd          Non-DM  \n",
       "3   blogandpostid/blog/post/1383-608ca73a171509000659ce03              DM  \n",
       "4                      blogandpostid/blog/post/1016-15628          Non-DM  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(zi_nats.shape)\n",
    "zi_nats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:  (26531, 4)\n",
      "After:  (6958, 4)\n"
     ]
    }
   ],
   "source": [
    "cols_to_keep = ['content_natid', 'Doc','Topic', 'final_topic_labels']\n",
    "\n",
    "may3_9_topics = pd.concat([may03_full_pred[cols_to_keep], \n",
    "                           may04_full_pred[cols_to_keep], \n",
    "                           may05_full_pred[cols_to_keep], \n",
    "                           may06_full_pred[cols_to_keep], \n",
    "                           may07_full_pred[cols_to_keep], \n",
    "                           may08_full_pred[cols_to_keep], \n",
    "                           may09_full_pred[cols_to_keep]])\n",
    "\n",
    "print(\"Before: \", may3_9_topics.shape)\n",
    "\n",
    "may3_9_topics.drop_duplicates(\"content_natid\", keep='last', inplace=True)\n",
    "\n",
    "print(\"After: \", may3_9_topics.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "297277"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(zi_nats[zi_nats.GA_cmsNaturalId.isin(may3_9_topics.content_natid)].GA_fullVisitorId.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before (729427, 7)\n",
      "After (543526, 7)\n",
      "Unique fullvids for eda:  246229\n",
      "Unique natids for eda:  4951\n"
     ]
    }
   ],
   "source": [
    "eda_df = pd.merge(zi_nats, \n",
    "                  may3_9_topics, \n",
    "                  left_on=\"GA_cmsNaturalId\", right_on=\"content_natid\", how=\"inner\")\n",
    "print(\"Before\", eda_df.shape)\n",
    "\n",
    "# remove noise articles\n",
    "eda_df = eda_df[eda_df.Topic!=-1]\n",
    "print(\"After\", eda_df.shape)\n",
    "print(\"Unique fullvids for eda: \", len(eda_df.GA_fullVisitorId.unique()))\n",
    "\n",
    "print(\"Unique natids for eda: \", len(eda_df.content_natid.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eda_df['keywords_lst'] = eda_df.final_topic_labels.str.split(',')\n",
    "# eda_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(153448, 7)\n",
      "(390078, 7)\n",
      "(6915, 7)\n"
     ]
    }
   ],
   "source": [
    "c_levels = eda_df[eda_df.managementLevel ==  'DM']  # 'C-level'\n",
    "non_cs = eda_df[eda_df.managementLevel == 'Non-DM']  # 'Non-Clevel'\n",
    "\n",
    "print(c_levels.shape)\n",
    "print(non_cs.shape)\n",
    "print(c_levels[c_levels.GA_fullVisitorId.isin(non_cs.GA_fullVisitorId)].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dogecoin, bitcoin price, ethereum, bitcoin cryptocurrency, cryptocurrency market, tesla     13138\n",
       "tax hikes, tax rate, bidens, trillion, american families, families plan                     10721\n",
       "app store, elefherious, fleeceware, apps, epic games, ios                                   10012\n",
       "dominion, giuliani, election, lindell, trump, lawsuit                                        7328\n",
       "india, deaths, doses, israel, vaccinated, vaccine                                            6281\n",
       "apps, chrome, floc, facebook, users, browser                                                 5939\n",
       "mars, nasa, astronauts, rocket, collins, moon                                                5706\n",
       "senate, statehood, democrats, republicans, filibuster, 25th amendment                        5540\n",
       "jenner, mar lago, grenell, election, wright, recall                                          4739\n",
       "astrazeneca vaccine, johnson johnson, blood clots, vaccines, johnson vaccine, 19 vaccine     3494\n",
       "Name: final_topic_labels, dtype: int64"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_levels.final_topic_labels.value_counts()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dogecoin, bitcoin price, ethereum, bitcoin cryptocurrency, cryptocurrency market, tesla         24418\n",
       "tax hikes, tax rate, bidens, trillion, american families, families plan                         20776\n",
       "app store, elefherious, fleeceware, apps, epic games, ios                                       19421\n",
       "damage, pokemon, players, outriders, enemies, weapon                                            17907\n",
       "apps, chrome, floc, facebook, users, browser                                                    17207\n",
       "loan cancellation, student loans, loan forgiveness, loan borrowers, cancel student, congress    13928\n",
       "dominion, giuliani, election, lindell, trump, lawsuit                                           13067\n",
       "mars, nasa, astronauts, rocket, collins, moon                                                   11763\n",
       "foundation, giving focus, trumps, runcie, cuomo, estimated                                      10424\n",
       "india, deaths, doses, israel, vaccinated, vaccine                                                9817\n",
       "Name: final_topic_labels, dtype: int64"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_cs.final_topic_labels.value_counts()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training vs Prediction EDA check for my sanity \n",
    "\n",
    "* Non-noise, Noise ratio is almost same in totally new unseen articles as it is in seen(trained) ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4199, 15)\n",
      "(3672, 15)\n",
      "(527, 15)\n",
      "\n",
      "% Repeating from training 0.87\n",
      "% Repeating from training 0.13\n"
     ]
    }
   ],
   "source": [
    "print(may03_full_pred.shape)\n",
    "print(m3_in_train.shape)\n",
    "print(m3_not_in_train.shape)\n",
    "\n",
    "print()\n",
    "m3_in_train = may03_full_pred[may03_full_pred.content_natid.isin(full_train_topics.content_natid)]\n",
    "m3_not_in_train = may03_full_pred[~may03_full_pred.content_natid.isin(full_train_topics.content_natid)]\n",
    "\n",
    "print(\"% Repeating from training\", round(m3_in_train.shape[0]/may03_full_pred.shape[0], 2))\n",
    "print(\"% Repeating from training\", round(m3_not_in_train.shape[0]/may03_full_pred.shape[0], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In train - got topic 0.77\n",
      "In train - got noise 0.23\n"
     ]
    }
   ],
   "source": [
    "print(\"In train - got topic\", round(m3_in_train[m3_in_train.Topic!=-1].shape[0]/m3_in_train.shape[0], 2))\n",
    "print(\"In train - got noise\", round(m3_in_train[m3_in_train.Topic==-1].shape[0]/m3_in_train.shape[0], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not in train - got topic 0.71\n",
      "Not in train - got noise 0.29\n"
     ]
    }
   ],
   "source": [
    "print(\"Not in train - got topic\", round(m3_not_in_train[m3_not_in_train.Topic!=-1].shape[0]/m3_not_in_train.shape[0], 2))\n",
    "print(\"Not in train - got noise\", round(m3_not_in_train[m3_not_in_train.Topic==-1].shape[0]/m3_not_in_train.shape[0], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noise (1011, 15)\n",
      "0.85\n",
      "0.15\n"
     ]
    }
   ],
   "source": [
    "m3_noise = may03_full_pred[may03_full_pred.Topic==-1]\n",
    "print(\"Noise\", m3_noise.shape)\n",
    "\n",
    "# 85% of noise were also noise in training data. 15% of predicted noise was new articles\n",
    "print(round(m3_in_train[m3_in_train.Topic==-1].shape[0]/m3_noise.shape[0], 2))\n",
    "print(round(m3_not_in_train[m3_not_in_train.Topic==-1].shape[0]/m3_noise.shape[0], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noise (3188, 15)\n",
      "0.88\n",
      "0.12\n"
     ]
    }
   ],
   "source": [
    "m3_nonnoise = may03_full_pred[may03_full_pred.Topic!=-1]\n",
    "print(\"Noise\", m3_nonnoise.shape)\n",
    "\n",
    "# 88% of non-noise were also non-noise in training data. rest 12% of non-noise were unseen articles\n",
    "print(round(m3_in_train[m3_in_train.Topic!=-1].shape[0]/m3_nonnoise.shape[0], 2))\n",
    "print(round(m3_not_in_train[m3_not_in_train.Topic!=-1].shape[0]/m3_nonnoise.shape[0], 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* When both have topics\n",
    "    * 90% articles get same topic in training & prediction <br><br>\n",
    "    \n",
    "* When either has noise\n",
    "    * We are gaining topics on noise articles more than we are losing by predicting on May3 i.e. \n",
    "    * training noise, pred non-noise -gained-588. pred noise, training non-noise -lost-278"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3672, 7)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_1 = ['content_natid', 'Doc', 'Topic', 'final_topic_labels']\n",
    "col_2= ['content_natid', 'Doc', 'Topic', 'topic_words']\n",
    "\n",
    "m3_train = pd.merge(may03_full_pred[col_1], full_train_topics[col_2], on=\"content_natid\", how=\"inner\")\n",
    "m3_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2357, 11)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_train_topics[full_train_topics.Topic==-1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---Both have topics---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "common - both have topics (2224, 7)\n",
      "common - training noise, pred noise (1989, 7) 0.89\n",
      "common - training noise, pred noise (235, 7) 0.11\n"
     ]
    }
   ],
   "source": [
    "print(\"common - both have topics\", m3_train[(m3_train.Topic_x != -1) & (m3_train.Topic_y != -1)].shape)\n",
    "print(\"common - training noise, pred noise\", m3_train[(m3_train.Topic_x != -1) & (m3_train.Topic_y != -1) \n",
    "                                                      & (m3_train.Topic_x == m3_train.Topic_y)].shape, round(1989/2224, 2))\n",
    "\n",
    "print(\"common - training noise, pred noise\", m3_train[(m3_train.Topic_x != -1) & (m3_train.Topic_y != -1) \n",
    "                                                      & (m3_train.Topic_x != m3_train.Topic_y)].shape, round(235/2224, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---Either has noise---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "either has noise (1448, 7)\n"
     ]
    }
   ],
   "source": [
    "print(\"either has noise\",  m3_train[(m3_train.Topic_x == -1) | (m3_train.Topic_y == -1)].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "common - training noise (1170, 7)\n",
      "common - training noise, pred non-noise (588, 7)\n",
      "common - training noise, pred noise (582, 7)\n"
     ]
    }
   ],
   "source": [
    "print(\"common - training noise\", m3_train[m3_train.Topic_y==-1].shape)\n",
    "print(\"common - training noise, pred non-noise\", m3_train[(m3_train.Topic_x != -1) & (m3_train.Topic_y == -1)].shape)\n",
    "print(\"common - training noise, pred noise\", m3_train[(m3_train.Topic_x == -1) & (m3_train.Topic_y == -1)].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "common - may 3 pred noise (860, 7)\n",
      "common - may 3 pred noise, training non-noise (278, 7)\n",
      "common - may 3 pred noise, training noise (582, 7)\n"
     ]
    }
   ],
   "source": [
    "print(\"common - may 3 pred noise\", m3_train[m3_train.Topic_x==-1].shape)\n",
    "print(\"common - may 3 pred noise, training non-noise\", m3_train[(m3_train.Topic_x == -1) & (m3_train.Topic_y != -1)].shape)\n",
    "print(\"common - may 3 pred noise, training noise\", m3_train[(m3_train.Topic_x == -1) & (m3_train.Topic_y == -1)].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day-over-Day EDA check for my sanity \n",
    "\n",
    "* When both have topics\n",
    "    * 82% articles get same topic day-over-day \n",
    "    * remaining 18% either topic could be right - borderline ones it seems \n",
    "        * all this will do: change topic size or scoring<br><br>\n",
    "    \n",
    "* When either has noise\n",
    "    * 50% both have same noise\n",
    "    * rest 50% - almost same gain/loss - seems borderline ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3422, 7)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m3_4 = pd.merge(may03_full_pred[col_1], may04_full_pred[col_1], on=\"content_natid\", how=\"inner\")\n",
    "m3_4.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---Both have topics---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "common - both have topics (2380, 7)\n",
      "common - both same topics (1949, 7) 0.82\n",
      "common - both diff topics (431, 7) 0.18\n"
     ]
    }
   ],
   "source": [
    "print(\"common - both have topics\", m3_4[(m3_4.Topic_x != -1) & (m3_4.Topic_y != -1)].shape)\n",
    "print(\"common - both same topics\", m3_4[(m3_4.Topic_x != -1) & (m3_4.Topic_y != -1) \n",
    "                                                      & (m3_4.Topic_x == m3_4.Topic_y)].shape, round(1949/2380, 2))\n",
    "\n",
    "print(\"common - both diff topics\", m3_4[(m3_4.Topic_x != -1) & (m3_4.Topic_y != -1) \n",
    "                                                      & (m3_4.Topic_x != m3_4.Topic_y)].shape, round(431/2380, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc_x</th>\n",
       "      <th>Topic_x</th>\n",
       "      <th>final_topic_labels_x</th>\n",
       "      <th>Topic_y</th>\n",
       "      <th>final_topic_labels_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the new ipad pro 2021 gets even better with tons of new features announced at wwdc. it was just over a year ago that apple unveiled its last ipad pro. the 2020 model wasnt a significant improvement over 2018sit mainly added support for the laptop...</td>\n",
       "      <td>26</td>\n",
       "      <td>iphone xs, iphone xr, iphone plus, iphone 6s, xs max, vs iphone</td>\n",
       "      <td>68</td>\n",
       "      <td>m1, rtx 3060, laptop, usb, intel, rx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>blockchain could transform retail, from supply chain and inventory management to product provenance. cryptocurrency and blockchain have been inching closer to the mainstream, with mastercard ma ma and paypal pypl pypl announcing theyll accept tok...</td>\n",
       "      <td>216</td>\n",
       "      <td>open innovation, digitalization, gpt, gartner, big data, superminds</td>\n",
       "      <td>244</td>\n",
       "      <td>supply chain, multicloud, iot, tiktok, hyperautomation, analytics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                        Doc_x  \\\n",
       "2   the new ipad pro 2021 gets even better with tons of new features announced at wwdc. it was just over a year ago that apple unveiled its last ipad pro. the 2020 model wasnt a significant improvement over 2018sit mainly added support for the laptop...   \n",
       "34  blockchain could transform retail, from supply chain and inventory management to product provenance. cryptocurrency and blockchain have been inching closer to the mainstream, with mastercard ma ma and paypal pypl pypl announcing theyll accept tok...   \n",
       "\n",
       "    Topic_x  \\\n",
       "2        26   \n",
       "34      216   \n",
       "\n",
       "                                                   final_topic_labels_x  \\\n",
       "2       iphone xs, iphone xr, iphone plus, iphone 6s, xs max, vs iphone   \n",
       "34  open innovation, digitalization, gpt, gartner, big data, superminds   \n",
       "\n",
       "    Topic_y                                               final_topic_labels_y  \n",
       "2        68                               m1, rtx 3060, laptop, usb, intel, rx  \n",
       "34      244  supply chain, multicloud, iot, tiktok, hyperautomation, analytics  "
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m3_4[(m3_4.Topic_x != -1) & (m3_4.Topic_y != -1) & (m3_4.Topic_x != m3_4.Topic_y)][['Doc_x', 'Topic_x', \n",
    "                                                                                    'final_topic_labels_x', 'Topic_y',\n",
    "                                                                                    'final_topic_labels_y']].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---Either has noise---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "either have noise (1042, 7)\n",
      "both noise (565, 7)\n",
      "may3 noise, may 4 non-noise (239, 7)\n",
      "may3 non-noise, may 4 noise (238, 7)\n",
      "[False]\n"
     ]
    }
   ],
   "source": [
    "print(\"either have noise\", m3_4[(m3_4.Topic_x==-1) | (m3_4.Topic_y==-1)].shape)\n",
    "print(\"both noise\", m3_4[(m3_4.Topic_x == -1) & (m3_4.Topic_y == -1)].shape)\n",
    "print(\"may3 noise, may 4 non-noise\", m3_4[(m3_4.Topic_x != -1) & (m3_4.Topic_y == -1)].shape)\n",
    "print(\"may3 non-noise, may 4 noise\", m3_4[(m3_4.Topic_x == -1) & (m3_4.Topic_y != -1)].shape)\n",
    "print(m3_4[(m3_4.Topic_x != -1) & \n",
    "           (m3_4.Topic_y == -1)].content_natid.isin(m3_4[(m3_4.Topic_x == -1) & \n",
    "                                                         (m3_4.Topic_y != -1)].content_natid).unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2249, 7)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m3_9 = pd.merge(may03_full_pred[col_1], may09_full_pred[col_1], on=\"content_natid\", how=\"inner\")\n",
    "m3_9.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "common - both have topics (1549, 7)\n",
      "common - both same topics (1281, 7) 0.82\n",
      "common - both diff topics (268, 7) 0.18\n"
     ]
    }
   ],
   "source": [
    "print(\"common - both have topics\", m3_9[(m3_9.Topic_x != -1) & (m3_9.Topic_y != -1)].shape)\n",
    "print(\"common - both same topics\", m3_9[(m3_9.Topic_x != -1) & (m3_9.Topic_y != -1) \n",
    "                                                      & (m3_9.Topic_x == m3_9.Topic_y)].shape, round(1949/2380, 2))\n",
    "\n",
    "print(\"common - both diff topics\", m3_9[(m3_9.Topic_x != -1) & (m3_9.Topic_y != -1) \n",
    "                                                      & (m3_9.Topic_x != m3_9.Topic_y)].shape, round(431/2380, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "either have noise (700, 7)\n",
      "both noise (373, 7)\n",
      "may3 noise, may 9 non-noise (177, 7)\n",
      "may3 non-noise, may 9 noise (150, 7)\n",
      "[False]\n"
     ]
    }
   ],
   "source": [
    "print(\"either have noise\", m3_9[(m3_9.Topic_x==-1) | (m3_9.Topic_y==-1)].shape)\n",
    "print(\"both noise\", m3_9[(m3_9.Topic_x == -1) & (m3_9.Topic_y == -1)].shape)\n",
    "print(\"may3 noise, may 9 non-noise\", m3_9[(m3_9.Topic_x != -1) & (m3_9.Topic_y == -1)].shape)\n",
    "print(\"may3 non-noise, may 9 noise\", m3_9[(m3_9.Topic_x == -1) & (m3_9.Topic_y != -1)].shape)\n",
    "print(m3_9[(m3_9.Topic_x != -1) & \n",
    "           (m3_9.Topic_y == -1)].content_natid.isin(m3_9[(m3_9.Topic_x == -1) & \n",
    "                                                         (m3_9.Topic_y != -1)].content_natid).unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replicate_prod_deduplication(df_list, raw_df):\n",
    "    \n",
    "    \"\"\"\n",
    "    merge earlier days dfs and use that to deduplicate current day's articles \n",
    "    \n",
    "    this function replicates a DB where unique articles and their topic assignments are stored. \n",
    "    \n",
    "    scope of look back period - past 7 days\n",
    "    \"\"\"\n",
    "\n",
    "    concat_dfs = pd.concat(df_list)\n",
    "    concat_dfs.drop_duplicates(\"content_natid\", keep=\"first\", inplace=True)\n",
    "    \n",
    "    updated_df = raw_df[~((raw_df.content_natid.isin(full_train_topics.content_natid_x)) | (raw_df.content_natid.isin(concat_dfs.content_natid)))]\n",
    "    updated_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return updated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7238, 13)\n"
     ]
    }
   ],
   "source": [
    "print(full_train_topics.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(450, 9)\n",
      "(237, 9)\n",
      "actual prediction noise 0.5266666666666666\n",
      "\n",
      "Reduced down to:\n",
      "(127, 9)\n",
      "actual prediction noise 0.2822222222222222\n"
     ]
    }
   ],
   "source": [
    "opt2_may5 = replicate_prod_deduplication([may03_full_pred, may04_full_pred], \n",
    "                                         may05_full_pred) \n",
    "\n",
    "print(opt2_may5.shape)\n",
    "\n",
    "print(opt2_may5[opt2_may5.orig_pred_topic==-1].shape)\n",
    "print(\"actual prediction noise\", opt2_may5[opt2_may5.orig_pred_topic==-1].shape[0]/opt2_may5.shape[0])\n",
    "\n",
    "print()\n",
    "print(\"Reduced down to:\")\n",
    "print(opt2_may5[opt2_may5.final_topics==-1].shape)\n",
    "print(\"actual prediction noise\", opt2_may5[opt2_may5.final_topics==-1].shape[0]/opt2_may5.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(452, 9)\n",
      "(246, 9)\n",
      "actual prediction noise 0.5442477876106194\n",
      "\n",
      "Reduced down to:\n",
      "(145, 9)\n",
      "actual prediction noise 0.32079646017699115\n"
     ]
    }
   ],
   "source": [
    "opt2_may6 = replicate_prod_deduplication([may03_full_pred, may04_full_pred, may05_full_pred], \n",
    "                                         may06_full_pred) \n",
    "\n",
    "print(opt2_may6.shape)\n",
    "\n",
    "print(opt2_may6[opt2_may6.orig_pred_topic==-1].shape)\n",
    "print(\"actual prediction noise\", opt2_may6[opt2_may6.orig_pred_topic==-1].shape[0]/opt2_may6.shape[0])\n",
    "\n",
    "print()\n",
    "print(\"Reduced down to:\")\n",
    "print(opt2_may6[opt2_may6.final_topics==-1].shape)\n",
    "print(\"actual prediction noise\", opt2_may6[opt2_may6.final_topics==-1].shape[0]/opt2_may6.shape[0])"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m59"
  },
  "kernelspec": {
   "display_name": "topics-env-latest",
   "language": "python",
   "name": "topics-env-latest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
